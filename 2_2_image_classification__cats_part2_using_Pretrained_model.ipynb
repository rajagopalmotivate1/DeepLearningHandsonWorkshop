{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.2 image_classification__cats_part2 using Pretrained model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "jTEzoMx6CasV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajagopalmotivate1/DeepLearningHandsonWorkshop/blob/master/2_2_image_classification__cats_part2_using_Pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jTEzoMx6CasV"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "8a6kxghEnBwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Credits : Google LLC\n",
        "\n",
        "Modified for Karunya workshop"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IhmPj1VVCfWb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YHK6DyunSbs4"
      },
      "cell_type": "markdown",
      "source": [
        "# Cat vs. Dog Image Classification with Inception\n",
        "## Exercise 3: Feature Extraction and Fine-Tuning\n",
        "**_Estimated completion time: 30 minutes_**\n",
        "\n",
        "In Exercise 1, we built a convnet from scratch, and were able to achieve an accuracy of about 70%. With the addition of data augmentation and dropout in Exercise 2, we were able to increase accuracy to about 80%. That seems decent, but 20% is still too high of an error rate. Maybe we just don't have enough training data available to properly solve the problem. What other approaches can we try?\n",
        "\n",
        "In this exercise, we'll look at two techniques for repurposing feature data generated from image models that have already been trained on large sets of data, **feature extraction** and **fine tuning**, and use them to improve the accuracy of our cat vs. dog classification model."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "dI5rmt4UBwXs"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction Using a Pretrained Model\n",
        "\n",
        "One thing that is commonly done in computer vision is to take a model trained on a very large dataset, run it on your own, smaller dataset, and extract the intermediate representations (features) that the model generates. These representations are frequently informative for your own computer vision task, even though the task may be quite different from the problem that the original model was trained on. This versatility and repurposability of convnets is one of the most interesting aspects of deep learning.\n",
        "\n",
        "In our case, we will use the [Inception V3 model](https://arxiv.org/abs/1512.00567) developed at Google, and pre-trained on [ImageNet](http://image-net.org/), a large dataset of web images (1.4M images and 1000 classes). This is a powerful model; let's see what the features that it has learned can do for our cat vs. dog problem.\n",
        "\n",
        "First, we need to pick which intermediate layer of Inception V3 we will use for feature extraction. A common practice is to use the output of the very last layer before the `Flatten` operation, the so-called \"bottleneck layer.\" The reasoning here is that the following fully connected layers will be too specialized for the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
        "\n",
        "Let's instantiate an Inception V3 model preloaded with weights trained on ImageNet:\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1xJZ5glPPCRz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VaXLMtYiF0t9"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's download the weights:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KMrbllgAFipZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c41805bf-f1b3-4e4c-b14e-4049373270d6"
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-04 16:44:42--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.206.128, 2a00:1450:400c:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.206.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "\r          /tmp/ince   0%[                    ]       0  --.-KB/s               \r         /tmp/incep  12%[=>                  ]  10.83M  54.1MB/s               \r        /tmp/incept  38%[======>             ]  32.01M  51.4MB/s               \r       /tmp/incepti  76%[==============>     ]  64.01M  76.8MB/s               \r/tmp/inception_v3_w 100%[===================>]  83.84M  91.9MB/s    in 0.9s    \n",
            "\n",
            "2019-03-04 16:44:44 (91.9 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UnRiGBfOF8rq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4c2c0641-640f-4aa0-fb33-4d562dcc4fdb"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "pre_trained_model = InceptionV3(\n",
        "    input_shape=(150, 150, 3), include_top=False, weights=None)\n",
        "pre_trained_model.load_weights(local_weights_file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IcYZPBS3bTAj"
      },
      "cell_type": "markdown",
      "source": [
        "By specifying the `include_top=False` argument, we load a network that doesn't include the classification layers at the top—ideal for feature extraction."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CFxrqTuJee5m"
      },
      "cell_type": "markdown",
      "source": [
        "Let's make the model non-trainable, since we will only use it for feature extraction; we won't update the weights of the pretrained model during training."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a38rB3lyedcB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XGBGDiOAepnO"
      },
      "cell_type": "markdown",
      "source": [
        "The layer we will use for feature extraction in Inception v3 is called `mixed7`. It is not the bottleneck of the network, but we are using it to keep a sufficiently large feature map (7x7 in this case). (Using the bottleneck layer would have resulting in a 3x3 feature map, which is a bit small.) Let's get the output from `mixed7`:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Cj4rXshqbQlS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a38753d6-bd59-4ceb-aa11-b8d8d23643b0"
      },
      "cell_type": "code",
      "source": [
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print 'last layer output shape:', last_layer.output_shape\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape: (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XxHk6XQLeUWh"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's stick a fully connected classifier on top of `last_output`:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BMXb913pbvFg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c0f81a2d-64a2-4ea3-f37e-bab4f7ff20c2"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2)(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Configure and compile the model\n",
        "model = Model(pre_trained_model.input, x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.0001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4G9h0PQykOGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten(name='Karunya1')(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu', name='Karunya2')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "x = layers.Dropout(0.2, name='Karunya3')(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid' , name='Karunya4')(x)\n",
        "\n",
        "# Configure and compile the model\n",
        "model = Model(pre_trained_model.input, x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.0001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7exLWIxhHmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8588
        },
        "outputId": "cfffd3c9-e11c-49d6-d7ff-ed412812262b"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1 (BatchNo (None, 74, 74, 32)   96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 74, 74, 32)   0           batch_normalization_v1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 72, 72, 32)   9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_1 (Batch (None, 72, 72, 32)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 72, 72, 32)   0           batch_normalization_v1_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 72, 72, 64)   18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_2 (Batch (None, 72, 72, 64)   192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 72, 72, 64)   0           batch_normalization_v1_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 35, 35, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 35, 35, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_3 (Batch (None, 35, 35, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 35, 35, 80)   0           batch_normalization_v1_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 33, 33, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_4 (Batch (None, 33, 33, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 33, 33, 192)  0           batch_normalization_v1_4[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_8 (Batch (None, 16, 16, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_v1_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_6 (Batch (None, 16, 16, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_9 (Batch (None, 16, 16, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 48)   0           batch_normalization_v1_6[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 96)   0           batch_normalization_v1_9[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_5 (Batch (None, 16, 16, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_7 (Batch (None, 16, 16, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_10 (Batc (None, 16, 16, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_11 (Batc (None, 16, 16, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_v1_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_v1_7[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_v1_11[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_15 (Batc (None, 16, 16, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_15[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_13 (Batc (None, 16, 16, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_16 (Batc (None, 16, 16, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 48)   0           batch_normalization_v1_13[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_16[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_12 (Batc (None, 16, 16, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_14 (Batc (None, 16, 16, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_17 (Batc (None, 16, 16, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_18 (Batc (None, 16, 16, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_12[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_14[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_17[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_18[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_22 (Batc (None, 16, 16, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_22[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_20 (Batc (None, 16, 16, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_23 (Batc (None, 16, 16, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 48)   0           batch_normalization_v1_20[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_23[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_19 (Batc (None, 16, 16, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_21 (Batc (None, 16, 16, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_24 (Batc (None, 16, 16, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_25 (Batc (None, 16, 16, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_19[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_21[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_24[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_25[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_27 (Batc (None, 16, 16, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_27[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_28 (Batc (None, 16, 16, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 16, 16, 96)   0           batch_normalization_v1_28[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 7, 7, 96)     82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_26 (Batc (None, 7, 7, 384)    1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_29 (Batc (None, 7, 7, 96)     288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 7, 7, 384)    0           batch_normalization_v1_26[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 7, 7, 96)     0           batch_normalization_v1_29[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_34 (Batc (None, 7, 7, 128)    384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_34[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 7, 7, 128)    114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_35 (Batc (None, 7, 7, 128)    384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_35[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_31 (Batc (None, 7, 7, 128)    384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_36 (Batc (None, 7, 7, 128)    384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_31[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_36[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 7, 7, 128)    114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_32 (Batc (None, 7, 7, 128)    384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_37 (Batc (None, 7, 7, 128)    384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_32[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_v1_37[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 7, 7, 192)    172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 7, 7, 192)    172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_30 (Batc (None, 7, 7, 192)    576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_33 (Batc (None, 7, 7, 192)    576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_38 (Batc (None, 7, 7, 192)    576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_39 (Batc (None, 7, 7, 192)    576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_30[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_33[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_38[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_39[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_44 (Batc (None, 7, 7, 160)    480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_44[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 7, 7, 160)    179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_45 (Batc (None, 7, 7, 160)    480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_45[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_41 (Batc (None, 7, 7, 160)    480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_46 (Batc (None, 7, 7, 160)    480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_41[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_46[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 7, 7, 160)    179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_42 (Batc (None, 7, 7, 160)    480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_47 (Batc (None, 7, 7, 160)    480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_42[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_47[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 7, 7, 192)    215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 7, 7, 192)    215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_40 (Batc (None, 7, 7, 192)    576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_43 (Batc (None, 7, 7, 192)    576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_48 (Batc (None, 7, 7, 192)    576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_49 (Batc (None, 7, 7, 192)    576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_40[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_43[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_48[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_49[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_54 (Batc (None, 7, 7, 160)    480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_54[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 7, 7, 160)    179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_55 (Batc (None, 7, 7, 160)    480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_55[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_51 (Batc (None, 7, 7, 160)    480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_56 (Batc (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_51[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_56[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 7, 7, 160)    179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_52 (Batc (None, 7, 7, 160)    480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_57 (Batc (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_52[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_v1_57[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 7, 7, 192)    215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 7, 7, 192)    215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_50 (Batc (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_53 (Batc (None, 7, 7, 192)    576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_58 (Batc (None, 7, 7, 192)    576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_59 (Batc (None, 7, 7, 192)    576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_50[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_53[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_58[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_59[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_64 (Batc (None, 7, 7, 192)    576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_64[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 7, 7, 192)    258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_65 (Batc (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_65[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_61 (Batc (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_66 (Batc (None, 7, 7, 192)    576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_61[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_66[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 7, 7, 192)    258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_62 (Batc (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_67 (Batc (None, 7, 7, 192)    576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_62[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_67[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_60 (Batc (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_63 (Batc (None, 7, 7, 192)    576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_68 (Batc (None, 7, 7, 192)    576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_69 (Batc (None, 7, 7, 192)    576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_60[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_63[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_68[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_v1_69[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Karunya1 (Flatten)              (None, 37632)        0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Karunya2 (Dense)                (None, 1024)         38536192    Karunya1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Karunya3 (Dropout)              (None, 1024)         0           Karunya2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "Karunya4 (Dense)                (None, 1)            1025        Karunya3[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 47,512,481\n",
            "Trainable params: 38,537,217\n",
            "Non-trainable params: 8,975,264\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GT3p9IcxhurH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8154ea6-d514-4817-986c-395879bba922"
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "from google.colab import files\n",
        "\n",
        "plot_model(model, show_shapes=False, show_layer_names=True, to_file='mymodelwithInception.png')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHocmjOrjI2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('mymodelwithInception.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lA0wqptRk68l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4043
        },
        "outputId": "3d8ff1d9-2a22-4b8a-f45a-859282e90b0a"
      },
      "cell_type": "code",
      "source": [
        "for alayer in model.layers:\n",
        "  print(str(alayer.name) + '    \\t \\t \\t ' +  str(alayer.trainable))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1    \t \t \t False\n",
            "conv2d    \t \t \t False\n",
            "batch_normalization_v1    \t \t \t False\n",
            "activation    \t \t \t False\n",
            "conv2d_1    \t \t \t False\n",
            "batch_normalization_v1_1    \t \t \t False\n",
            "activation_1    \t \t \t False\n",
            "conv2d_2    \t \t \t False\n",
            "batch_normalization_v1_2    \t \t \t False\n",
            "activation_2    \t \t \t False\n",
            "max_pooling2d    \t \t \t False\n",
            "conv2d_3    \t \t \t False\n",
            "batch_normalization_v1_3    \t \t \t False\n",
            "activation_3    \t \t \t False\n",
            "conv2d_4    \t \t \t False\n",
            "batch_normalization_v1_4    \t \t \t False\n",
            "activation_4    \t \t \t False\n",
            "max_pooling2d_1    \t \t \t False\n",
            "conv2d_8    \t \t \t False\n",
            "batch_normalization_v1_8    \t \t \t False\n",
            "activation_8    \t \t \t False\n",
            "conv2d_6    \t \t \t False\n",
            "conv2d_9    \t \t \t False\n",
            "batch_normalization_v1_6    \t \t \t False\n",
            "batch_normalization_v1_9    \t \t \t False\n",
            "activation_6    \t \t \t False\n",
            "activation_9    \t \t \t False\n",
            "average_pooling2d    \t \t \t False\n",
            "conv2d_5    \t \t \t False\n",
            "conv2d_7    \t \t \t False\n",
            "conv2d_10    \t \t \t False\n",
            "conv2d_11    \t \t \t False\n",
            "batch_normalization_v1_5    \t \t \t False\n",
            "batch_normalization_v1_7    \t \t \t False\n",
            "batch_normalization_v1_10    \t \t \t False\n",
            "batch_normalization_v1_11    \t \t \t False\n",
            "activation_5    \t \t \t False\n",
            "activation_7    \t \t \t False\n",
            "activation_10    \t \t \t False\n",
            "activation_11    \t \t \t False\n",
            "mixed0    \t \t \t False\n",
            "conv2d_15    \t \t \t False\n",
            "batch_normalization_v1_15    \t \t \t False\n",
            "activation_15    \t \t \t False\n",
            "conv2d_13    \t \t \t False\n",
            "conv2d_16    \t \t \t False\n",
            "batch_normalization_v1_13    \t \t \t False\n",
            "batch_normalization_v1_16    \t \t \t False\n",
            "activation_13    \t \t \t False\n",
            "activation_16    \t \t \t False\n",
            "average_pooling2d_1    \t \t \t False\n",
            "conv2d_12    \t \t \t False\n",
            "conv2d_14    \t \t \t False\n",
            "conv2d_17    \t \t \t False\n",
            "conv2d_18    \t \t \t False\n",
            "batch_normalization_v1_12    \t \t \t False\n",
            "batch_normalization_v1_14    \t \t \t False\n",
            "batch_normalization_v1_17    \t \t \t False\n",
            "batch_normalization_v1_18    \t \t \t False\n",
            "activation_12    \t \t \t False\n",
            "activation_14    \t \t \t False\n",
            "activation_17    \t \t \t False\n",
            "activation_18    \t \t \t False\n",
            "mixed1    \t \t \t False\n",
            "conv2d_22    \t \t \t False\n",
            "batch_normalization_v1_22    \t \t \t False\n",
            "activation_22    \t \t \t False\n",
            "conv2d_20    \t \t \t False\n",
            "conv2d_23    \t \t \t False\n",
            "batch_normalization_v1_20    \t \t \t False\n",
            "batch_normalization_v1_23    \t \t \t False\n",
            "activation_20    \t \t \t False\n",
            "activation_23    \t \t \t False\n",
            "average_pooling2d_2    \t \t \t False\n",
            "conv2d_19    \t \t \t False\n",
            "conv2d_21    \t \t \t False\n",
            "conv2d_24    \t \t \t False\n",
            "conv2d_25    \t \t \t False\n",
            "batch_normalization_v1_19    \t \t \t False\n",
            "batch_normalization_v1_21    \t \t \t False\n",
            "batch_normalization_v1_24    \t \t \t False\n",
            "batch_normalization_v1_25    \t \t \t False\n",
            "activation_19    \t \t \t False\n",
            "activation_21    \t \t \t False\n",
            "activation_24    \t \t \t False\n",
            "activation_25    \t \t \t False\n",
            "mixed2    \t \t \t False\n",
            "conv2d_27    \t \t \t False\n",
            "batch_normalization_v1_27    \t \t \t False\n",
            "activation_27    \t \t \t False\n",
            "conv2d_28    \t \t \t False\n",
            "batch_normalization_v1_28    \t \t \t False\n",
            "activation_28    \t \t \t False\n",
            "conv2d_26    \t \t \t False\n",
            "conv2d_29    \t \t \t False\n",
            "batch_normalization_v1_26    \t \t \t False\n",
            "batch_normalization_v1_29    \t \t \t False\n",
            "activation_26    \t \t \t False\n",
            "activation_29    \t \t \t False\n",
            "max_pooling2d_2    \t \t \t False\n",
            "mixed3    \t \t \t False\n",
            "conv2d_34    \t \t \t False\n",
            "batch_normalization_v1_34    \t \t \t False\n",
            "activation_34    \t \t \t False\n",
            "conv2d_35    \t \t \t False\n",
            "batch_normalization_v1_35    \t \t \t False\n",
            "activation_35    \t \t \t False\n",
            "conv2d_31    \t \t \t False\n",
            "conv2d_36    \t \t \t False\n",
            "batch_normalization_v1_31    \t \t \t False\n",
            "batch_normalization_v1_36    \t \t \t False\n",
            "activation_31    \t \t \t False\n",
            "activation_36    \t \t \t False\n",
            "conv2d_32    \t \t \t False\n",
            "conv2d_37    \t \t \t False\n",
            "batch_normalization_v1_32    \t \t \t False\n",
            "batch_normalization_v1_37    \t \t \t False\n",
            "activation_32    \t \t \t False\n",
            "activation_37    \t \t \t False\n",
            "average_pooling2d_3    \t \t \t False\n",
            "conv2d_30    \t \t \t False\n",
            "conv2d_33    \t \t \t False\n",
            "conv2d_38    \t \t \t False\n",
            "conv2d_39    \t \t \t False\n",
            "batch_normalization_v1_30    \t \t \t False\n",
            "batch_normalization_v1_33    \t \t \t False\n",
            "batch_normalization_v1_38    \t \t \t False\n",
            "batch_normalization_v1_39    \t \t \t False\n",
            "activation_30    \t \t \t False\n",
            "activation_33    \t \t \t False\n",
            "activation_38    \t \t \t False\n",
            "activation_39    \t \t \t False\n",
            "mixed4    \t \t \t False\n",
            "conv2d_44    \t \t \t False\n",
            "batch_normalization_v1_44    \t \t \t False\n",
            "activation_44    \t \t \t False\n",
            "conv2d_45    \t \t \t False\n",
            "batch_normalization_v1_45    \t \t \t False\n",
            "activation_45    \t \t \t False\n",
            "conv2d_41    \t \t \t False\n",
            "conv2d_46    \t \t \t False\n",
            "batch_normalization_v1_41    \t \t \t False\n",
            "batch_normalization_v1_46    \t \t \t False\n",
            "activation_41    \t \t \t False\n",
            "activation_46    \t \t \t False\n",
            "conv2d_42    \t \t \t False\n",
            "conv2d_47    \t \t \t False\n",
            "batch_normalization_v1_42    \t \t \t False\n",
            "batch_normalization_v1_47    \t \t \t False\n",
            "activation_42    \t \t \t False\n",
            "activation_47    \t \t \t False\n",
            "average_pooling2d_4    \t \t \t False\n",
            "conv2d_40    \t \t \t False\n",
            "conv2d_43    \t \t \t False\n",
            "conv2d_48    \t \t \t False\n",
            "conv2d_49    \t \t \t False\n",
            "batch_normalization_v1_40    \t \t \t False\n",
            "batch_normalization_v1_43    \t \t \t False\n",
            "batch_normalization_v1_48    \t \t \t False\n",
            "batch_normalization_v1_49    \t \t \t False\n",
            "activation_40    \t \t \t False\n",
            "activation_43    \t \t \t False\n",
            "activation_48    \t \t \t False\n",
            "activation_49    \t \t \t False\n",
            "mixed5    \t \t \t False\n",
            "conv2d_54    \t \t \t False\n",
            "batch_normalization_v1_54    \t \t \t False\n",
            "activation_54    \t \t \t False\n",
            "conv2d_55    \t \t \t False\n",
            "batch_normalization_v1_55    \t \t \t False\n",
            "activation_55    \t \t \t False\n",
            "conv2d_51    \t \t \t False\n",
            "conv2d_56    \t \t \t False\n",
            "batch_normalization_v1_51    \t \t \t False\n",
            "batch_normalization_v1_56    \t \t \t False\n",
            "activation_51    \t \t \t False\n",
            "activation_56    \t \t \t False\n",
            "conv2d_52    \t \t \t False\n",
            "conv2d_57    \t \t \t False\n",
            "batch_normalization_v1_52    \t \t \t False\n",
            "batch_normalization_v1_57    \t \t \t False\n",
            "activation_52    \t \t \t False\n",
            "activation_57    \t \t \t False\n",
            "average_pooling2d_5    \t \t \t False\n",
            "conv2d_50    \t \t \t False\n",
            "conv2d_53    \t \t \t False\n",
            "conv2d_58    \t \t \t False\n",
            "conv2d_59    \t \t \t False\n",
            "batch_normalization_v1_50    \t \t \t False\n",
            "batch_normalization_v1_53    \t \t \t False\n",
            "batch_normalization_v1_58    \t \t \t False\n",
            "batch_normalization_v1_59    \t \t \t False\n",
            "activation_50    \t \t \t False\n",
            "activation_53    \t \t \t False\n",
            "activation_58    \t \t \t False\n",
            "activation_59    \t \t \t False\n",
            "mixed6    \t \t \t False\n",
            "conv2d_64    \t \t \t False\n",
            "batch_normalization_v1_64    \t \t \t False\n",
            "activation_64    \t \t \t False\n",
            "conv2d_65    \t \t \t False\n",
            "batch_normalization_v1_65    \t \t \t False\n",
            "activation_65    \t \t \t False\n",
            "conv2d_61    \t \t \t False\n",
            "conv2d_66    \t \t \t False\n",
            "batch_normalization_v1_61    \t \t \t False\n",
            "batch_normalization_v1_66    \t \t \t False\n",
            "activation_61    \t \t \t False\n",
            "activation_66    \t \t \t False\n",
            "conv2d_62    \t \t \t False\n",
            "conv2d_67    \t \t \t False\n",
            "batch_normalization_v1_62    \t \t \t False\n",
            "batch_normalization_v1_67    \t \t \t False\n",
            "activation_62    \t \t \t False\n",
            "activation_67    \t \t \t False\n",
            "average_pooling2d_6    \t \t \t False\n",
            "conv2d_60    \t \t \t False\n",
            "conv2d_63    \t \t \t False\n",
            "conv2d_68    \t \t \t False\n",
            "conv2d_69    \t \t \t False\n",
            "batch_normalization_v1_60    \t \t \t False\n",
            "batch_normalization_v1_63    \t \t \t False\n",
            "batch_normalization_v1_68    \t \t \t False\n",
            "batch_normalization_v1_69    \t \t \t False\n",
            "activation_60    \t \t \t False\n",
            "activation_63    \t \t \t False\n",
            "activation_68    \t \t \t False\n",
            "activation_69    \t \t \t False\n",
            "mixed7    \t \t \t False\n",
            "Karunya1    \t \t \t True\n",
            "Karunya2    \t \t \t True\n",
            "Karunya3    \t \t \t True\n",
            "Karunya4    \t \t \t True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_6ECjowwV5Ug"
      },
      "cell_type": "markdown",
      "source": [
        "For examples and data preprocessing, let's use the same files and `train_generator` as we did in Exercise 2."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Cl-IqOTjZVw_"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE:** The 2,000 images used in this exercise are excerpted from the [\"Dogs vs. Cats\" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O4s8HckqGlnb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "377c4f2e-828d-4eec-8201-39bb0652fff5"
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O \\\n",
        "   /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-04 16:45:11--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.206.128, 2a00:1450:400c:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.206.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  54.8MB/s    in 1.2s    \n",
            "\n",
            "2019-03-04 16:45:12 (54.8 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Fl9XXARuV_eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d0811d3-cd24-4e51-8026-c141a8dee3cc"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir, # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qEC1AL7iVRLz"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, let's train the model using the features we extracted. We'll train on all 2000 images available, for 2 epochs, and validate on all 1,000 test images."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Blhq2MAUeyGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "51cbc060-896a-492d-e5b6-c96b78f158ad"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=2,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/2\n",
            "50/50 [==============================] - 7s 134ms/step - loss: 0.2678 - acc: 0.9160\n",
            " - 28s - loss: 0.5077 - acc: 0.7650 - val_loss: 0.2678 - val_acc: 0.9160\n",
            "Epoch 2/2\n",
            "50/50 [==============================] - 6s 123ms/step - loss: 0.2415 - acc: 0.9380\n",
            " - 25s - loss: 0.3830 - acc: 0.8310 - val_loss: 0.2415 - val_acc: 0.9380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lRjyAkE62aOG"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that we reach a validation accuracy of 88–90% very quickly. This is much better than the small model we trained from scratch."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tt15y6IS2pBo"
      },
      "cell_type": "markdown",
      "source": [
        "## Further Improving Accuracy with Fine-Tuning\n",
        "\n",
        "In our feature-extraction experiment, we only tried adding two classification layers on top of an Inception V3 layer. The weights of the pretrained network were not updated during training. One way to increase performance even further is to \"fine-tune\" the weights of the top layers of the pretrained model alongside the training of the top-level classifier. A couple of important notes on fine-tuning:\n",
        "\n",
        "- **Fine-tuning should only be attempted *after* you have trained the top-level classifier with the pretrained model set to non-trainable**. If you add a randomly initialized classifier on top of a pretrained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier), and your pretrained model will just forget everything it has learned.\n",
        "- Additionally, we **fine-tune only the *top layers* of the pre-trained model** rather than all layers of the pretrained model because, in a convnet, the higher up a layer is, the more specialized it is. The first few layers in a convnet learn very simple and generic features, which generalize to almost all types of images. But as you go higher up, the features are increasingly specific to the dataset that the model is trained on. The goal of fine-tuning is to adapt these specialized features to work with the new dataset.\n",
        "\n",
        "All we need to do to implement fine-tuning is to set the top layers of Inception V3 to be trainable, recompile the model (necessary for these changes to take effect), and resume training. Let's unfreeze all layers belonging to the `mixed7` module—i.e., all layers found after `mixed6`—and recompile the model:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_l_J4S0Z2rgg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "unfreeze = False\n",
        "\n",
        "# Unfreeze all models after \"mixed6\"\n",
        "for layer in pre_trained_model.layers:\n",
        "  if unfreeze:\n",
        "    layer.trainable = True\n",
        "  if layer.name == 'mixed6':\n",
        "    unfreeze = True\n",
        "\n",
        "# As an optimizer, here we will use SGD \n",
        "# with a very low learning rate (0.00001)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=SGD(\n",
        "                  lr=0.00001, \n",
        "                  momentum=0.9),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VqfPOKilzFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4043
        },
        "outputId": "29432c9c-5bc6-4843-f832-c38a78fc6de5"
      },
      "cell_type": "code",
      "source": [
        "for alayer in model.layers:\n",
        "  print(str(alayer.name) + '    \\t \\t \\t ' +  str(alayer.trainable))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_1    \t \t \t False\n",
            "conv2d    \t \t \t False\n",
            "batch_normalization_v1    \t \t \t False\n",
            "activation    \t \t \t False\n",
            "conv2d_1    \t \t \t False\n",
            "batch_normalization_v1_1    \t \t \t False\n",
            "activation_1    \t \t \t False\n",
            "conv2d_2    \t \t \t False\n",
            "batch_normalization_v1_2    \t \t \t False\n",
            "activation_2    \t \t \t False\n",
            "max_pooling2d    \t \t \t False\n",
            "conv2d_3    \t \t \t False\n",
            "batch_normalization_v1_3    \t \t \t False\n",
            "activation_3    \t \t \t False\n",
            "conv2d_4    \t \t \t False\n",
            "batch_normalization_v1_4    \t \t \t False\n",
            "activation_4    \t \t \t False\n",
            "max_pooling2d_1    \t \t \t False\n",
            "conv2d_8    \t \t \t False\n",
            "batch_normalization_v1_8    \t \t \t False\n",
            "activation_8    \t \t \t False\n",
            "conv2d_6    \t \t \t False\n",
            "conv2d_9    \t \t \t False\n",
            "batch_normalization_v1_6    \t \t \t False\n",
            "batch_normalization_v1_9    \t \t \t False\n",
            "activation_6    \t \t \t False\n",
            "activation_9    \t \t \t False\n",
            "average_pooling2d    \t \t \t False\n",
            "conv2d_5    \t \t \t False\n",
            "conv2d_7    \t \t \t False\n",
            "conv2d_10    \t \t \t False\n",
            "conv2d_11    \t \t \t False\n",
            "batch_normalization_v1_5    \t \t \t False\n",
            "batch_normalization_v1_7    \t \t \t False\n",
            "batch_normalization_v1_10    \t \t \t False\n",
            "batch_normalization_v1_11    \t \t \t False\n",
            "activation_5    \t \t \t False\n",
            "activation_7    \t \t \t False\n",
            "activation_10    \t \t \t False\n",
            "activation_11    \t \t \t False\n",
            "mixed0    \t \t \t False\n",
            "conv2d_15    \t \t \t False\n",
            "batch_normalization_v1_15    \t \t \t False\n",
            "activation_15    \t \t \t False\n",
            "conv2d_13    \t \t \t False\n",
            "conv2d_16    \t \t \t False\n",
            "batch_normalization_v1_13    \t \t \t False\n",
            "batch_normalization_v1_16    \t \t \t False\n",
            "activation_13    \t \t \t False\n",
            "activation_16    \t \t \t False\n",
            "average_pooling2d_1    \t \t \t False\n",
            "conv2d_12    \t \t \t False\n",
            "conv2d_14    \t \t \t False\n",
            "conv2d_17    \t \t \t False\n",
            "conv2d_18    \t \t \t False\n",
            "batch_normalization_v1_12    \t \t \t False\n",
            "batch_normalization_v1_14    \t \t \t False\n",
            "batch_normalization_v1_17    \t \t \t False\n",
            "batch_normalization_v1_18    \t \t \t False\n",
            "activation_12    \t \t \t False\n",
            "activation_14    \t \t \t False\n",
            "activation_17    \t \t \t False\n",
            "activation_18    \t \t \t False\n",
            "mixed1    \t \t \t False\n",
            "conv2d_22    \t \t \t False\n",
            "batch_normalization_v1_22    \t \t \t False\n",
            "activation_22    \t \t \t False\n",
            "conv2d_20    \t \t \t False\n",
            "conv2d_23    \t \t \t False\n",
            "batch_normalization_v1_20    \t \t \t False\n",
            "batch_normalization_v1_23    \t \t \t False\n",
            "activation_20    \t \t \t False\n",
            "activation_23    \t \t \t False\n",
            "average_pooling2d_2    \t \t \t False\n",
            "conv2d_19    \t \t \t False\n",
            "conv2d_21    \t \t \t False\n",
            "conv2d_24    \t \t \t False\n",
            "conv2d_25    \t \t \t False\n",
            "batch_normalization_v1_19    \t \t \t False\n",
            "batch_normalization_v1_21    \t \t \t False\n",
            "batch_normalization_v1_24    \t \t \t False\n",
            "batch_normalization_v1_25    \t \t \t False\n",
            "activation_19    \t \t \t False\n",
            "activation_21    \t \t \t False\n",
            "activation_24    \t \t \t False\n",
            "activation_25    \t \t \t False\n",
            "mixed2    \t \t \t False\n",
            "conv2d_27    \t \t \t False\n",
            "batch_normalization_v1_27    \t \t \t False\n",
            "activation_27    \t \t \t False\n",
            "conv2d_28    \t \t \t False\n",
            "batch_normalization_v1_28    \t \t \t False\n",
            "activation_28    \t \t \t False\n",
            "conv2d_26    \t \t \t False\n",
            "conv2d_29    \t \t \t False\n",
            "batch_normalization_v1_26    \t \t \t False\n",
            "batch_normalization_v1_29    \t \t \t False\n",
            "activation_26    \t \t \t False\n",
            "activation_29    \t \t \t False\n",
            "max_pooling2d_2    \t \t \t False\n",
            "mixed3    \t \t \t False\n",
            "conv2d_34    \t \t \t False\n",
            "batch_normalization_v1_34    \t \t \t False\n",
            "activation_34    \t \t \t False\n",
            "conv2d_35    \t \t \t False\n",
            "batch_normalization_v1_35    \t \t \t False\n",
            "activation_35    \t \t \t False\n",
            "conv2d_31    \t \t \t False\n",
            "conv2d_36    \t \t \t False\n",
            "batch_normalization_v1_31    \t \t \t False\n",
            "batch_normalization_v1_36    \t \t \t False\n",
            "activation_31    \t \t \t False\n",
            "activation_36    \t \t \t False\n",
            "conv2d_32    \t \t \t False\n",
            "conv2d_37    \t \t \t False\n",
            "batch_normalization_v1_32    \t \t \t False\n",
            "batch_normalization_v1_37    \t \t \t False\n",
            "activation_32    \t \t \t False\n",
            "activation_37    \t \t \t False\n",
            "average_pooling2d_3    \t \t \t False\n",
            "conv2d_30    \t \t \t False\n",
            "conv2d_33    \t \t \t False\n",
            "conv2d_38    \t \t \t False\n",
            "conv2d_39    \t \t \t False\n",
            "batch_normalization_v1_30    \t \t \t False\n",
            "batch_normalization_v1_33    \t \t \t False\n",
            "batch_normalization_v1_38    \t \t \t False\n",
            "batch_normalization_v1_39    \t \t \t False\n",
            "activation_30    \t \t \t False\n",
            "activation_33    \t \t \t False\n",
            "activation_38    \t \t \t False\n",
            "activation_39    \t \t \t False\n",
            "mixed4    \t \t \t False\n",
            "conv2d_44    \t \t \t False\n",
            "batch_normalization_v1_44    \t \t \t False\n",
            "activation_44    \t \t \t False\n",
            "conv2d_45    \t \t \t False\n",
            "batch_normalization_v1_45    \t \t \t False\n",
            "activation_45    \t \t \t False\n",
            "conv2d_41    \t \t \t False\n",
            "conv2d_46    \t \t \t False\n",
            "batch_normalization_v1_41    \t \t \t False\n",
            "batch_normalization_v1_46    \t \t \t False\n",
            "activation_41    \t \t \t False\n",
            "activation_46    \t \t \t False\n",
            "conv2d_42    \t \t \t False\n",
            "conv2d_47    \t \t \t False\n",
            "batch_normalization_v1_42    \t \t \t False\n",
            "batch_normalization_v1_47    \t \t \t False\n",
            "activation_42    \t \t \t False\n",
            "activation_47    \t \t \t False\n",
            "average_pooling2d_4    \t \t \t False\n",
            "conv2d_40    \t \t \t False\n",
            "conv2d_43    \t \t \t False\n",
            "conv2d_48    \t \t \t False\n",
            "conv2d_49    \t \t \t False\n",
            "batch_normalization_v1_40    \t \t \t False\n",
            "batch_normalization_v1_43    \t \t \t False\n",
            "batch_normalization_v1_48    \t \t \t False\n",
            "batch_normalization_v1_49    \t \t \t False\n",
            "activation_40    \t \t \t False\n",
            "activation_43    \t \t \t False\n",
            "activation_48    \t \t \t False\n",
            "activation_49    \t \t \t False\n",
            "mixed5    \t \t \t False\n",
            "conv2d_54    \t \t \t False\n",
            "batch_normalization_v1_54    \t \t \t False\n",
            "activation_54    \t \t \t False\n",
            "conv2d_55    \t \t \t False\n",
            "batch_normalization_v1_55    \t \t \t False\n",
            "activation_55    \t \t \t False\n",
            "conv2d_51    \t \t \t False\n",
            "conv2d_56    \t \t \t False\n",
            "batch_normalization_v1_51    \t \t \t False\n",
            "batch_normalization_v1_56    \t \t \t False\n",
            "activation_51    \t \t \t False\n",
            "activation_56    \t \t \t False\n",
            "conv2d_52    \t \t \t False\n",
            "conv2d_57    \t \t \t False\n",
            "batch_normalization_v1_52    \t \t \t False\n",
            "batch_normalization_v1_57    \t \t \t False\n",
            "activation_52    \t \t \t False\n",
            "activation_57    \t \t \t False\n",
            "average_pooling2d_5    \t \t \t False\n",
            "conv2d_50    \t \t \t False\n",
            "conv2d_53    \t \t \t False\n",
            "conv2d_58    \t \t \t False\n",
            "conv2d_59    \t \t \t False\n",
            "batch_normalization_v1_50    \t \t \t False\n",
            "batch_normalization_v1_53    \t \t \t False\n",
            "batch_normalization_v1_58    \t \t \t False\n",
            "batch_normalization_v1_59    \t \t \t False\n",
            "activation_50    \t \t \t False\n",
            "activation_53    \t \t \t False\n",
            "activation_58    \t \t \t False\n",
            "activation_59    \t \t \t False\n",
            "mixed6    \t \t \t False\n",
            "conv2d_64    \t \t \t True\n",
            "batch_normalization_v1_64    \t \t \t True\n",
            "activation_64    \t \t \t True\n",
            "conv2d_65    \t \t \t True\n",
            "batch_normalization_v1_65    \t \t \t True\n",
            "activation_65    \t \t \t True\n",
            "conv2d_61    \t \t \t True\n",
            "conv2d_66    \t \t \t True\n",
            "batch_normalization_v1_61    \t \t \t True\n",
            "batch_normalization_v1_66    \t \t \t True\n",
            "activation_61    \t \t \t True\n",
            "activation_66    \t \t \t True\n",
            "conv2d_62    \t \t \t True\n",
            "conv2d_67    \t \t \t True\n",
            "batch_normalization_v1_62    \t \t \t True\n",
            "batch_normalization_v1_67    \t \t \t True\n",
            "activation_62    \t \t \t True\n",
            "activation_67    \t \t \t True\n",
            "average_pooling2d_6    \t \t \t True\n",
            "conv2d_60    \t \t \t True\n",
            "conv2d_63    \t \t \t True\n",
            "conv2d_68    \t \t \t True\n",
            "conv2d_69    \t \t \t True\n",
            "batch_normalization_v1_60    \t \t \t True\n",
            "batch_normalization_v1_63    \t \t \t True\n",
            "batch_normalization_v1_68    \t \t \t True\n",
            "batch_normalization_v1_69    \t \t \t True\n",
            "activation_60    \t \t \t True\n",
            "activation_63    \t \t \t True\n",
            "activation_68    \t \t \t True\n",
            "activation_69    \t \t \t True\n",
            "mixed7    \t \t \t True\n",
            "Karunya1    \t \t \t True\n",
            "Karunya2    \t \t \t True\n",
            "Karunya3    \t \t \t True\n",
            "Karunya4    \t \t \t True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zE37ARlqY9da"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's retrain the model. We'll train on all 2000 images available, for 50 epochs, and validate on all 1,000 test images. (This may take 15-20 minutes to run.)"
      ]
    },
    {
      "metadata": {
        "id": "EO2WK_mJh6rv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "myIterations = 3\n",
        "#ideal is around 20 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "o_GgDGG4Y_hJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7c40fd51-eb7f-4395-a777-e4fe5c240701"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=myIterations,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,\n",
        "      verbose=2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "50/50 [==============================] - 7s 137ms/step - loss: 0.2447 - acc: 0.9420\n",
            " - 28s - loss: 0.2722 - acc: 0.8825 - val_loss: 0.2447 - val_acc: 0.9420\n",
            "Epoch 2/3\n",
            "50/50 [==============================] - 7s 132ms/step - loss: 0.2466 - acc: 0.9440\n",
            " - 26s - loss: 0.3044 - acc: 0.8690 - val_loss: 0.2466 - val_acc: 0.9440\n",
            "Epoch 3/3\n",
            "50/50 [==============================] - 6s 125ms/step - loss: 0.2472 - acc: 0.9440\n",
            " - 26s - loss: 0.2804 - acc: 0.8740 - val_loss: 0.2472 - val_acc: 0.9440\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3EPGn58ofwq5"
      },
      "cell_type": "markdown",
      "source": [
        "We are seeing a nice improvement, with the validation loss going from ~1.7 down to ~1.2, and accuracy going from 88% to 92%. That's a 4.5% relative improvement in accuracy.\n",
        "\n",
        "Let's plot the training and validation loss and accuracy to show it conclusively:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1FtxcKjJfxL9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "outputId": "3966e2f0-bcf7-49b7-c1f5-fbce177c83cd"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Retrieve a list of accuracy results on training and test data\n",
        "# sets for each training epoch\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(acc))\n",
        "\n",
        "# Plot training and validation accuracy per epoch\n",
        "plt.plot(epochs, acc)\n",
        "plt.plot(epochs, val_acc)\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plot training and validation loss per epoch\n",
        "plt.plot(epochs, loss)\n",
        "plt.plot(epochs, val_loss)\n",
        "plt.title('Training and validation loss')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Training and validation loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "X-fUIeizakjE"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations! Using feature extraction and fine-tuning, you've built an image classification model that can identify cats vs. dogs in images with over 90% accuracy."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "x_ANwJCnx7w-"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean Up\n",
        "\n",
        "Run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-hUmyohAyBzh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import os, signal\n",
        "#os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}